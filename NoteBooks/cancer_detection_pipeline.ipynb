{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ab83f6",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from tensorflow.keras.models import Model, save_model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "Path(\"../models/supervised\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"../models/unsupervised\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"../models/preprocessing\").mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc98466c",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14617af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data validation utilities\n",
    "def validate_dataframe(df, name=\"DataFrame\"):\n",
    "    \"\"\"Validate DataFrame quality and report statistics.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Validation: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "    print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "    print(f\"Data types: {df.dtypes.value_counts().to_dict()}\")\n",
    "    \n",
    "    if df.select_dtypes(include=[np.number]).shape[1] > 0:\n",
    "        numeric_df = df.select_dtypes(include=[np.number])\n",
    "        print(f\"Numeric columns: {numeric_df.shape[1]}\")\n",
    "        print(f\"  - Mean range: [{numeric_df.mean().min():.2e}, {numeric_df.mean().max():.2e}]\")\n",
    "        print(f\"  - Std range: [{numeric_df.std().min():.2e}, {numeric_df.std().max():.2e}]\")\n",
    "    \n",
    "    print(f\"{'='*50}\\n\")\n",
    "    return df\n",
    "\n",
    "def check_class_balance(y, label_name=\"Labels\"):\n",
    "    \"\"\"Check and report class distribution.\"\"\"\n",
    "    counts = y.value_counts()\n",
    "    print(f\"\\n{label_name} distribution:\")\n",
    "    for label, count in counts.items():\n",
    "        percentage = (count / len(y)) * 100\n",
    "        print(f\"  {label}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Check for severe imbalance\n",
    "    if len(counts) > 1:\n",
    "        ratio = counts.max() / counts.min()\n",
    "        if ratio > 3:\n",
    "            print(f\"⚠ Warning: Class imbalance detected (ratio {ratio:.1f}:1)\")\n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93671a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use relative path from notebooks/ to data/\n",
    "file_path = Path(\"../data/GSE19804_series_matrix.txt\")\n",
    "\n",
    "# Check if file exists\n",
    "if not file_path.exists():\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "    print(f\"Please ensure the data file is in the data/ directory\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Lire les lignes utiles\n",
    "try:\n",
    "    with open(file_path, 'rt') as f:\n",
    "        lines = f.readlines()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading file: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Identify start and end lines of the data matrix\n",
    "start = None\n",
    "end = None\n",
    "for i, line in enumerate(lines):\n",
    "    if line.startswith(\"!series_matrix_table_begin\"):  # start of our matrix\n",
    "        start = i + 1  # ignore metadata line\n",
    "    elif line.startswith(\"!series_matrix_table_end\"):  # end of our matrix\n",
    "        end = i\n",
    "        break\n",
    "\n",
    "if start is None or end is None:\n",
    "    print(\"Error: Could not find data matrix markers in file\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# read the data matrix\n",
    "df = pd.read_csv(file_path, sep=\"\\t\", skiprows=start, nrows=end - start - 1, index_col=0, comment=\"!\")\n",
    "print(f\"✓ Data loaded successfully: {df.shape[0]} genes, {df.shape[1]} samples\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bc7085",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Samples: {df.columns[:5]}\")\n",
    "\n",
    "# Remove columns with missing names\n",
    "df = df.loc[:, df.columns.notna()]\n",
    "print(f\"✓ Removed columns with missing names: {df.shape}\")\n",
    "\n",
    "# Remove duplicate gene rows\n",
    "df = df[~df.index.duplicated(keep='first')]\n",
    "print(f\"✓ Removed duplicate genes: {df.shape}\")\n",
    "\n",
    "# Convert all values to numeric\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Check for missing values\n",
    "missing_count = df.isnull().sum().sum()\n",
    "print(f\"⚠ Missing values found: {missing_count}\")\n",
    "\n",
    "if missing_count > 0:\n",
    "    # Count missing per row and column\n",
    "    missing_per_row = df.isnull().sum(axis=1)\n",
    "    missing_per_col = df.isnull().sum(axis=0)\n",
    "    print(f\"  - Genes with missing values: {(missing_per_row > 0).sum()}\")\n",
    "    print(f\"  - Samples with missing values: {(missing_per_col > 0).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1358bb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy: Remove genes (rows) with any missing values\n",
    "# Keep all samples (columns) since they're our observations\n",
    "df_clean = df.dropna(axis=0)  # axis=0 removes rows (genes) with NaN\n",
    "print(f\"✓ Removed genes with missing values: {df.shape[0]} → {df_clean.shape[0]} genes\")\n",
    "\n",
    "# Remove genes (columns after transpose) with all zeros (no expression)\n",
    "df_clean = df_clean.loc[:, (df_clean != 0).any(axis=0)]\n",
    "print(f\"✓ Removed zero-expression genes: {df_clean.shape}\")\n",
    "\n",
    "# Transpose: samples as rows, genes as columns\n",
    "df_T = df_clean.T\n",
    "print(f\"✓ Transposed data: {df_T.shape[0]} samples × {df_T.shape[1]} genes\")\n",
    "\n",
    "# Standardize features (Z-score normalization)\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_T), \n",
    "    index=df_T.index, \n",
    "    columns=df_T.columns\n",
    ")\n",
    "\n",
    "# Verify cleaning\n",
    "print(f\"\\n✓ Final cleaned data: {df_scaled.shape}\")\n",
    "print(f\"✓ Missing values: {df_scaled.isnull().sum().sum()}\")\n",
    "print(f\"✓ Mean: {df_scaled.mean().mean():.2e} (should be ~0)\")\n",
    "print(f\"✓ Std: {df_scaled.std().mean():.2f} (should be ~1)\")\n",
    "\n",
    "# Save scaler and feature names for later use\n",
    "scaler_path = Path(\"../models/preprocessing/scaler.pkl\")\n",
    "features_path = Path(\"../models/preprocessing/feature_names.pkl\")\n",
    "\n",
    "joblib.dump(scaler, scaler_path)\n",
    "joblib.dump(list(df_scaled.columns), features_path)\n",
    "print(f\"\\n✓ Saved scaler to {scaler_path}\")\n",
    "print(f\"✓ Saved feature names to {features_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480ec51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced to 2 dimensions for visualization\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Explained variance\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "print(f\"Variance explained by PC1: {explained_var[0]:.2%}\")\n",
    "print(f\"Variance explained by PC2: {explained_var[1]:.2%}\")\n",
    "print(f\"Total variance explained: {explained_var.sum():.2%}\")\n",
    "\n",
    "# Store the projection with samples index\n",
    "df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'], index=df_scaled.index)\n",
    "print(f\"\\n✓ PCA projection: {df_pca.shape}\")\n",
    "\n",
    "# Save PCA transformer\n",
    "pca_path = Path(\"../models/preprocessing/pca_transformer.pkl\")\n",
    "joblib.dump(pca, pca_path)\n",
    "print(f\"✓ Saved PCA transformer to {pca_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319fc108",
   "metadata": {},
   "source": [
    "## Anomaly Detection with Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Isolation Forest model\n",
    "iso_forest = IsolationForest(n_estimators=100, contamination=0.1, random_state=42)\n",
    "\n",
    "# Train the model on standardized data\n",
    "iso_forest.fit(df_scaled)\n",
    "\n",
    "# Predict anomalies: -1 = anomaly, 1 = normal\n",
    "df_pca['Anomaly_IF'] = iso_forest.predict(df_scaled)\n",
    "\n",
    "# Save model\n",
    "if_path = Path(\"../models/unsupervised/isolation_forest.pkl\")\n",
    "joblib.dump(iso_forest, if_path)\n",
    "print(f\"✓ Saved Isolation Forest model to {if_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a571c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new text column for display\n",
    "df_pca['Anomaly_Label'] = df_pca['Anomaly_IF'].map({1: \"Normal\", -1: \"Anomaly\"})\n",
    "\n",
    "# Use this column for color\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=df_pca,  # DataFrame with principal components and anomaly labels\n",
    "    x='PC1', y='PC2',  # Principal components for axes\n",
    "    hue='Anomaly_Label',  # Color based on anomaly label\n",
    "    palette={\"Normal\": \"blue\", \"Anomaly\": \"red\"},  # Color palette for labels\n",
    "    s=80  # Point size\n",
    ")\n",
    "\n",
    "plt.title(\"Anomaly Detection by Isolation Forest\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Predicted Status\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9883092b",
   "metadata": {},
   "source": [
    "## Anomaly Detection with Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b68392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build autoencoder architecture\n",
    "n_features = df_scaled.shape[1]\n",
    "\n",
    "input_layer = Input(shape=(n_features,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "output_layer = Dense(n_features, activation='linear')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "print(f\"Autoencoder architecture: {n_features} → 64 → 32 → 64 → {n_features}\")\n",
    "\n",
    "# Train autoencoder\n",
    "history = autoencoder.fit(\n",
    "    df_scaled, df_scaled,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    validation_split=0.1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"✓ Autoencoder training completed\")\n",
    "print(f\"  Final loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"  Final val_loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "# Save autoencoder model\n",
    "ae_path = Path(\"../models/unsupervised/autoencoder.h5\")\n",
    "autoencoder.save(ae_path)\n",
    "print(f\"✓ Saved Autoencoder model to {ae_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9912bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction\n",
    "reconstructed = autoencoder.predict(df_scaled)\n",
    "# Mean squared error per sample\n",
    "reconstruction_error = np.mean(np.square(df_scaled - reconstructed), axis=1)\n",
    "# Add to PCA DataFrame\n",
    "df_pca['Reconstruction_Error'] = reconstruction_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c4bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct df_pca with aligned index\n",
    "df_pca = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\"], index=df_scaled.index)\n",
    "\n",
    "# Add reconstruction error\n",
    "df_pca['Reconstruction_Error'] = reconstruction_error\n",
    "\n",
    "# Add Isolation Forest predictions\n",
    "df_pca['Anomaly_IF'] = iso_forest.predict(df_scaled).astype(int)\n",
    "\n",
    "# AE detection: threshold based on 90th percentile\n",
    "threshold = np.percentile(reconstruction_error, 90)\n",
    "\n",
    "# Anomaly detection: -1 = anomaly, 1 = normal\n",
    "df_pca['Anomaly_AE'] = (df_pca['Reconstruction_Error'] > threshold).astype(int)\n",
    "df_pca['Anomaly_AE'] = df_pca['Anomaly_AE'].map({0: 1, 1: -1})  # align with Isolation Forest\n",
    "\n",
    "# Add readable label\n",
    "df_pca['AE_Label'] = df_pca['Anomaly_AE'].map({1: \"Normal\", -1: \"Anomaly\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e5e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=df_pca,\n",
    "    x='PC1', y='PC2',\n",
    "    hue='AE_Label',\n",
    "    palette={\"Normal\": \"blue\", \"Anomaly\": \"red\"},\n",
    "    s=80\n",
    ")\n",
    "\n",
    "plt.title(\"Anomaly Detection by Autoencoder\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Predicted Status\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc6e6c7",
   "metadata": {},
   "source": [
    "## Anomaly Detection with Local Outlier Factor (LOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630eb126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LOF model\n",
    "lof = LocalOutlierFactor(\n",
    "    n_neighbors=20,\n",
    "    contamination=0.1,\n",
    "    novelty=False  # fit_predict mode only\n",
    ")\n",
    "\n",
    "# Predict anomalies directly (LOF doesn't have separate fit/predict)\n",
    "lof_preds = lof.fit_predict(df_scaled)  # -1 = anomaly, 1 = normal\n",
    "\n",
    "# Add to PCA dataframe\n",
    "df_pca['Anomaly_LOF'] = lof_preds \n",
    "df_pca['LOF_Label'] = df_pca['Anomaly_LOF'].map({1: \"Normal\", -1: \"Anomaly\"})\n",
    "\n",
    "print(f\"✓ LOF predictions completed\")\n",
    "\n",
    "# Note: LOF with novelty=False cannot be saved for prediction on new data\n",
    "# For deployment, use novelty=True and retrain\n",
    "print(\"⚠ LOF model not saved (novelty=False mode cannot predict on new data)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c35743",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=df_pca,\n",
    "    x='PC1', y='PC2',\n",
    "    hue='LOF_Label',\n",
    "    palette={\"Normal\": \"blue\", \"Anomaly\": \"red\"},\n",
    "    s=80\n",
    ")\n",
    "\n",
    "plt.title(\"Anomaly Detection by Local Outlier Factor\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Predicted Status\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453d2470",
   "metadata": {},
   "source": [
    "## Model Comparison Against Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sample metadata from original file\n",
    "sample_ids = []\n",
    "sample_labels = []\n",
    "\n",
    "# Extract sample IDs\n",
    "for line in lines:\n",
    "    if line.startswith(\"!Sample_geo_accession\"):\n",
    "        sample_ids = [x.replace('\"', '').strip() for x in line.strip().split(\"\\t\")[1:]]\n",
    "        break\n",
    "\n",
    "if not sample_ids:\n",
    "    print(\"⚠ Warning: No sample IDs found in file\")\n",
    "\n",
    "# Extract sample labels from titles\n",
    "for line in lines:\n",
    "    if line.startswith(\"!Sample_title\"):\n",
    "        titles = line.strip().split(\"\\t\")[1:]\n",
    "        for title in titles:\n",
    "            t = title.lower()\n",
    "            if \"normal\" in t:\n",
    "                sample_labels.append(\"Healthy\")\n",
    "            elif \"cancer\" in t or \"tumor\" in t:\n",
    "                sample_labels.append(\"Cancer\")\n",
    "            else:\n",
    "                sample_labels.append(\"Unknown\")\n",
    "        break\n",
    "\n",
    "if not sample_labels:\n",
    "    print(\"⚠ Warning: No sample labels found in file\")\n",
    "elif len(sample_ids) != len(sample_labels):\n",
    "    print(f\"⚠ Warning: Mismatch between IDs ({len(sample_ids)}) and labels ({len(sample_labels)})\")\n",
    "\n",
    "# Create mapping dictionary\n",
    "sample_status = dict(zip(sample_ids, sample_labels))\n",
    "\n",
    "# Map labels to dataframe\n",
    "df_pca['Label'] = df_pca.index.map(sample_status)\n",
    "\n",
    "# Check for unmapped samples\n",
    "unmapped = df_pca['Label'].isna().sum()\n",
    "if unmapped > 0:\n",
    "    print(f\"⚠ Warning: {unmapped} samples could not be mapped to labels\")\n",
    "\n",
    "# Display distribution\n",
    "print(\"\\n✓ Sample label distribution:\")\n",
    "print(df_pca['Label'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c086d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(anomaly_col, model_name):\n",
    "    \"\"\"Evaluate unsupervised anomaly detection model against true labels.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluation: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Prepare evaluation dataframe\n",
    "    df_eval = df_pca[['Label', anomaly_col]].copy()\n",
    "    \n",
    "    # Clean and standardize labels\n",
    "    df_eval['Label_clean'] = df_eval['Label'].astype(str).str.lower().str.strip()\n",
    "    df_eval['Label_clean'] = df_eval['Label_clean'].replace({\n",
    "        'cancer': 'Cancer',\n",
    "        'normal': 'Healthy', \n",
    "        'tumor': 'Cancer',\n",
    "        'healthy': 'Healthy'\n",
    "    })\n",
    "    \n",
    "    # Filter valid data only\n",
    "    data = df_eval[df_eval['Label_clean'].isin(['Cancer', 'Healthy'])].copy()\n",
    "    \n",
    "    if data.empty:\n",
    "        print(\"⚠ No usable data for evaluation\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Evaluating on {len(data)} samples\")\n",
    "    \n",
    "    # Convert to binary: 1 = Cancer, 0 = Healthy\n",
    "    y_true = data['Label_clean'].map({'Cancer': 1, 'Healthy': 0})\n",
    "    y_pred = data[anomaly_col].map({-1: 1, 1: 0})  # -1 (anomaly) = Cancer\n",
    "    \n",
    "    # Check for conversion issues\n",
    "    if y_pred.isna().any():\n",
    "        print(f\"⚠ Warning: {y_pred.isna().sum()} predictions could not be converted\")\n",
    "    \n",
    "    try:\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "        \n",
    "        # Classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_true, y_pred, target_names=[\"Healthy\", \"Cancer\"], zero_division=0))\n",
    "        \n",
    "        return {'confusion_matrix': cm, 'y_true': y_true, 'y_pred': y_pred}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error during evaluation: {e}\")\n",
    "        return None\n",
    "\n",
    "# Note: Call this function for each model\n",
    "# Example: evaluate_model('Anomaly_IF', 'Isolation Forest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d924e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model('Anomaly_IF', 'Isolation Forest')  # Evaluate Isolation Forest model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76e7dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model('Anomaly_AE', 'Autoencoder')  # Evaluate Autoencoder model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef2649",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model('Anomaly_LOF', 'Local Outlier Factor')  # Evaluate Local Outlier Factor model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16cb560",
   "metadata": {},
   "source": [
    "| Model                | Accuracy | Recall Cancer | Precision Cancer | F1-score Cancer |\n",
    "| -------------------- | -------- | ------------- | ---------------- | --------------- |\n",
    "| **Isolation Forest** | 57%      | 17%           | 83%              | 28%             |\n",
    "| **Autoencoder**      | 42%      | **2%**        | **8%**           | **3%**          |\n",
    "| **LOF**              | **53%**  | **13%**       | 67%              | **22%**         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a2a5ec",
   "metadata": {},
   "source": [
    "## Supervised Learning Pipeline\n",
    "\n",
    "### Data Preparation for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ecab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for supervised learning\n",
    "X = df_scaled.copy()\n",
    "\n",
    "# Extract and clean labels\n",
    "y = df_pca['Label'].astype(str).str.lower().str.strip()\n",
    "y = y.replace({'cancer': 1, 'healthy': 0, 'normal': 0})\n",
    "\n",
    "# Remove samples with unknown labels\n",
    "valid_mask = y.isin([0, 1])\n",
    "X = X[valid_mask]\n",
    "y = y[valid_mask].astype(int)\n",
    "\n",
    "print(f\"✓ Prepared supervised learning data:\")\n",
    "print(f\"  - Features: {X.shape}\")\n",
    "print(f\"  - Labels: {len(y)}\")\n",
    "\n",
    "# Check class balance\n",
    "check_class_balance(y, \"Class distribution\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1edc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split with stratification\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Train/test split successful:\")\n",
    "    print(f\"  - Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"  - Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    print(\"\\n  Training set distribution:\")\n",
    "    check_class_balance(y_train, \"Train\")\n",
    "    \n",
    "    print(\"\\n  Test set distribution:\")\n",
    "    check_class_balance(y_test, \"Test\")\n",
    "    \n",
    "except ValueError as e:\n",
    "    print(f\"✗ Error during train/test split: {e}\")\n",
    "    print(\"This may happen if one class has too few samples for stratification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009b6fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM model\n",
    "print(\"Training SVM (Linear Kernel)...\")\n",
    "\n",
    "try:\n",
    "    svm_model = SVC(kernel='linear', random_state=42)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_svm = svm_model.predict(X_test)\n",
    "    \n",
    "    print(\"✓ SVM training completed\")\n",
    "    \n",
    "    # Save model\n",
    "    svm_path = Path(\"../models/supervised/svm_model.pkl\")\n",
    "    joblib.dump(svm_model, svm_path)\n",
    "    print(f\"✓ Saved SVM model to {svm_path}\\n\")\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"=\"*60)\n",
    "    print(\"SVM Evaluation Results\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred_svm)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_svm, target_names=[\"Healthy\", \"Cancer\"], zero_division=0))\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "    print(f\"\\nTest Accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error training SVM: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313de8a8",
   "metadata": {},
   "source": [
    "### SVM Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dd5d7d",
   "metadata": {},
   "source": [
    "#### Comparison with Unsupervised Models\n",
    "\n",
    "| Metric           | Supervised SVM | IF (best unsupervised) |\n",
    "| ---------------- | -------------- | ---------------------- |\n",
    "| Recall Cancer    | **0.83**       | 0.17                   |\n",
    "| Precision Cancer | 0.91           | **0.83**               |\n",
    "| F1-score Cancer  | **0.87**       | 0.28                   |\n",
    "| Accuracy         | **0.88**       | 0.57                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f621e",
   "metadata": {},
   "source": [
    "### XGBoost Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83587051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "print(\"Training XGBoost Classifier...\")\n",
    "\n",
    "try:\n",
    "    xgb_model = XGBClassifier(\n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    \n",
    "    print(\"✓ XGBoost training completed\")\n",
    "    \n",
    "    # Save model\n",
    "    xgb_path = Path(\"../models/supervised/xgboost_model.pkl\")\n",
    "    joblib.dump(xgb_model, xgb_path)\n",
    "    print(f\"✓ Saved XGBoost model to {xgb_path}\\n\")\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"=\"*60)\n",
    "    print(\"XGBoost Evaluation Results\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred_xgb)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_xgb, target_names=[\"Healthy\", \"Cancer\"], zero_division=0))\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "    print(f\"\\nTest Accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error training XGBoost: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8588a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances\n",
    "importances = xgb_model.feature_importances_\n",
    "feature_names = X.columns  # gene names\n",
    "\n",
    "# Create sorted DataFrame\n",
    "feat_imp = pd.DataFrame({\n",
    "    'Gene': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False).head(20)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feat_imp, x='Importance', y='Gene', palette='viridis')\n",
    "plt.title(\"Top 20 Most Discriminative Genes According to XGBoost\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Gene\")\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7567fac6",
   "metadata": {},
   "source": [
    "### Model Predictions Visualization in PCA Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75fbf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to df_pca (on test set only)\n",
    "df_pca.loc[X_test.index, 'Pred_SVM'] = y_pred_svm  # values 0/1\n",
    "df_pca.loc[X_test.index, 'Pred_XGB'] = y_pred_xgb \n",
    "# Add columns with readable labels for SVM and XGBoost\n",
    "df_pca['SVM_Label'] = df_pca['Pred_SVM'].map({1: 'Cancer', 0: 'Healthy'}) \n",
    "df_pca['XGB_Label'] = df_pca['Pred_XGB'].map({1: 'Cancer', 0: 'Healthy'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3501343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=df_pca.loc[X_test.index],\n",
    "    x='PC1', y='PC2',\n",
    "    hue='SVM_Label',\n",
    "    palette={'Healthy': 'blue', 'Cancer': 'red'},\n",
    "    s=80\n",
    ")\n",
    "plt.title(\"SVM Model Predictions (on test data)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Predicted Class\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d001a567",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=df_pca.loc[X_test.index],\n",
    "    x='PC1', y='PC2',\n",
    "    hue='XGB_Label',\n",
    "    palette={'Healthy': 'blue', 'Cancer': 'red'},\n",
    "    s=80\n",
    ")\n",
    "plt.title(\"XGBoost Model Predictions (on test data)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Predicted Class\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e25432",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_genes = feat_imp['Gene'].tolist()[:20]  # top 20 genes\n",
    "\n",
    "# Extract their columns from df_scaled\n",
    "X_subset = df_scaled[top_genes]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = X_subset.corr()\n",
    "\n",
    "# Display with seaborn\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .6})\n",
    "plt.title(\"Correlation Matrix of the 20 Most Discriminative Genes (XGBoost)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
